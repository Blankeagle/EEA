{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Dataset creation finished. #Trees: 1\n",
      "the gorgeously elaborate continuation of `` the lord of the rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director peter jackson 's expanded vision of j.r.r. tolkien 's middle-earth . "
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Tree-LSTM structure was first introduced by Kai et. al in an ACL 2015 \n",
    "# paper: `Improved Semantic Representations From Tree-Structured Long\n",
    "# Short-Term Memory Networks <https://arxiv.org/pdf/1503.00075.pdf>`__.\n",
    "# The core idea is to introduce syntactic information for language tasks by \n",
    "# extending the chain-structured LSTM to a tree-structured LSTM. The Dependency \n",
    "# Tree/Constituency Tree techniques were leveraged to obtain a ''latent tree''.\n",
    "#\n",
    "# One, if not all, difficulty of training Tree-LSTMs is batching --- a standard \n",
    "# technique in machine learning to accelerate optimization. However, since trees \n",
    "# generally have different shapes by nature, parallization becomes non trivial. \n",
    "# DGL offers an alternative: to pool all the trees into one single graph then \n",
    "# induce the message passing over them guided by the structure of each tree.\n",
    "#\n",
    "# The task and the dataset\n",
    "# ------------------------\n",
    "# In this tutorial, we will use Tree-LSTMs for sentiment analysis.\n",
    "# We have wrapped the\n",
    "# `Stanford Sentiment Treebank <https://nlp.stanford.edu/sentiment/>`__ in\n",
    "# ``dgl.data``. The dataset provides a fine-grained tree level sentiment\n",
    "# annotation: 5 classes(very negative, negative, neutral, positive, and\n",
    "# very positive) that indicates the sentiment in current subtree. Non-leaf\n",
    "# nodes in constituency tree does not contain words, we use a special\n",
    "# ``PAD_WORD`` token to denote them, during the training/inferencing,\n",
    "# their embeddings would be masked to all-zero.\n",
    "#\n",
    "# .. figure:: https://i.loli.net/2018/11/08/5be3d4bfe031b.png\n",
    "#    :alt: \n",
    "#\n",
    "# The figure displays one sample of the SST dataset, which is a\n",
    "# constituency parse tree with their nodes labeled with sentiment. To\n",
    "# speed up things, let's build a tiny set with 5 sentences and take a look\n",
    "# at the first one:\n",
    "#\n",
    "\n",
    "import dgl\n",
    "from dgl.data.tree import SST\n",
    "from dgl.data import SSTBatch\n",
    "\n",
    "# Each sample in the dataset is a constituency tree. The leaf nodes\n",
    "# represent words. The word is a int value stored in the \"x\" field.\n",
    "# The non-leaf nodes has a special word PAD_WORD. The sentiment\n",
    "# label is stored in the \"y\" feature field.\n",
    "trainset = SST(mode='tiny')  # the \"tiny\" set has only 5 trees\n",
    "#trainset = SST(mode='tiny')  # the \"tiny\" set has only 5 trees\n",
    "tiny_sst = trainset.trees\n",
    "num_vocabs = trainset.num_vocabs\n",
    "num_classes = trainset.num_classes\n",
    "\n",
    "vocab = trainset.vocab # vocabulary dict: key -> id\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # inverted vocabulary dict: id -> word\n",
    "\n",
    "a_tree = tiny_sst[0]\n",
    "for token in a_tree.ndata['x'].tolist():\n",
    "    if token != trainset.PAD_WORD:\n",
    "        print(inv_vocab[token], end=\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Step 1: batching\n",
    "# ----------------\n",
    "#\n",
    "# The first step is to throw all the trees into one graph, using\n",
    "# the :func:`~dgl.batched_graph.batch` API.\n",
    "#\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = dgl.batch(tiny_sst)\n",
    "def plot_tree(g):\n",
    "    # this plot requires pygraphviz package\n",
    "    pos = nx.nx_agraph.graphviz_layout(g, prog='dot')\n",
    "    nx.draw(g, pos, with_labels=False, node_size=10,\n",
    "            node_color=[[.5, .5, .5]], arrowsize=4)\n",
    "    plt.show()\n",
    "\n",
    "plot_tree(graph.to_networkx())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# You can read more about the definition of :func:`~dgl.batched_graph.batch`\n",
    "# (by clicking the API), or can skip ahead to the next step:\n",
    "# \n",
    "# .. note::\n",
    "#\n",
    "#    **Definition**: a :class:`~dgl.batched_graph.BatchedDGLGraph` is a\n",
    "#    :class:`~dgl.DGLGraph` that unions a list of :class:`~dgl.DGLGraph`\\ s. \n",
    "#    \n",
    "#    - The union includes all the nodes,\n",
    "#      edges, and their features. The order of nodes, edges and features are\n",
    "#      preserved. \n",
    "#     \n",
    "#        - Given that we have :math:`V_i` nodes for graph\n",
    "#          :math:`\\mathcal{G}_i`, the node ID :math:`j` in graph\n",
    "#          :math:`\\mathcal{G}_i` correspond to node ID\n",
    "#          :math:`j + \\sum_{k=1}^{i-1} V_k` in the batched graph. \n",
    "#    \n",
    "#        - Therefore, performing feature transformation and message passing on\n",
    "#          ``BatchedDGLGraph`` is equivalent to doing those\n",
    "#          on all ``DGLGraph`` constituents in parallel. \n",
    "#\n",
    "#    - Duplicate references to the same graph are\n",
    "#      treated as deep copies; the nodes, edges, and features are duplicated,\n",
    "#      and mutation on one reference does not affect the other. \n",
    "#    - Currently, ``BatchedDGLGraph`` is immutable in\n",
    "#      graph structure (i.e. one can't add\n",
    "#      nodes and edges to it). We need to support mutable batched graphs in\n",
    "#      (far) future. \n",
    "#    - The ``BatchedDGLGraph`` keeps track of the meta\n",
    "#      information of the constituents so it can be\n",
    "#      :func:`~dgl.batched_graph.unbatch`\\ ed to list of ``DGLGraph``\\ s.\n",
    "#\n",
    "# For more details about the :class:`~dgl.batched_graph.BatchedDGLGraph`\n",
    "# module in DGL, you can click the class name.\n",
    "#\n",
    "# Step 2: Tree-LSTM Cell with message-passing APIs\n",
    "# ------------------------------------------------\n",
    "#\n",
    "# The authors proposed two types of Tree LSTM: Child-Sum\n",
    "# Tree-LSTMs, and :math:`N`-ary Tree-LSTMs. In this tutorial we focus \n",
    "# on applying *Binary* Tree-LSTM to binarized constituency trees(this \n",
    "# application is also known as *Constituency Tree-LSTM*). We use PyTorch \n",
    "# as our backend framework to set up the network.\n",
    "#\n",
    "# In `N`-ary Tree LSTM, each unit at node :math:`j` maintains a hidden\n",
    "# representation :math:`h_j` and a memory cell :math:`c_j`. The unit\n",
    "# :math:`j` takes the input vector :math:`x_j` and the hidden\n",
    "# representations of the their child units: :math:`h_{jl}, 1\\leq l\\leq N` as\n",
    "# input, then update its new hidden representation :math:`h_j` and memory\n",
    "# cell :math:`c_j` by: \n",
    "#\n",
    "# .. math::\n",
    "#\n",
    "#    i_j & = & \\sigma\\left(W^{(i)}x_j + \\sum_{l=1}^{N}U^{(i)}_l h_{jl} + b^{(i)}\\right),  & (1)\\\\\n",
    "#    f_{jk} & = & \\sigma\\left(W^{(f)}x_j + \\sum_{l=1}^{N}U_{kl}^{(f)} h_{jl} + b^{(f)} \\right), &  (2)\\\\\n",
    "#    o_j & = & \\sigma\\left(W^{(o)}x_j + \\sum_{l=1}^{N}U_{l}^{(o)} h_{jl} + b^{(o)} \\right), & (3)  \\\\\n",
    "#    u_j & = & \\textrm{tanh}\\left(W^{(u)}x_j + \\sum_{l=1}^{N} U_l^{(u)}h_{jl} + b^{(u)} \\right), & (4)\\\\\n",
    "#    c_j & = & i_j \\odot u_j + \\sum_{l=1}^{N} f_{jl} \\odot c_{jl}, &(5) \\\\\n",
    "#    h_j & = & o_j \\cdot \\textrm{tanh}(c_j), &(6)  \\\\\n",
    "#\n",
    "# It can be decomposed into three phases: ``message_func``,\n",
    "# ``reduce_func`` and ``apply_node_func``.\n",
    "#\n",
    "# .. note::\n",
    "#    ``apply_node_func`` is a new node UDF we have not introduced before. In\n",
    "#    ``apply_node_func``, user specifies what to do with node features,\n",
    "#    without considering edge features and messages. In Tree-LSTM case,\n",
    "#    ``apply_node_func`` is a must, since there exists (leaf) nodes with\n",
    "#    :math:`0` incoming edges, which would not be updated via\n",
    "#    ``reduce_func``.\n",
    "#\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class TreeLSTMCell(nn.Module):\n",
    "    def __init__(self, x_size, h_size):\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "        self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
    "        self.U_iou = nn.Linear(2 * h_size, 3 * h_size, bias=False)\n",
    "        self.b_iou = nn.Parameter(th.zeros(1, 3 * h_size))\n",
    "        self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {'h': edges.src['h'], 'c': edges.src['c']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # concatenate h_jl for equation (1), (2), (3), (4)\n",
    "        h_cat = nodes.mailbox['h'].view(nodes.mailbox['h'].size(0), -1)\n",
    "        # equation (2)\n",
    "        f = th.sigmoid(self.U_f(h_cat)).view(*nodes.mailbox['h'].size())\n",
    "        # second term of equation (5)\n",
    "        c = th.sum(f * nodes.mailbox['c'], 1)\n",
    "        return {'iou': self.U_iou(h_cat), 'c': c}\n",
    "\n",
    "    def apply_node_func(self, nodes):\n",
    "        # equation (1), (3), (4)\n",
    "        iou = nodes.data['iou'] + self.b_iou\n",
    "        i, o, u = th.chunk(iou, 3, 1)\n",
    "        i, o, u = th.sigmoid(i), th.sigmoid(o), th.tanh(u)\n",
    "        # equation (5)\n",
    "        c = i * u + nodes.data['c']\n",
    "        # equation (6)\n",
    "        h = o * th.tanh(c)\n",
    "        return {'h' : h, 'c' : c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traversing one tree:\n",
      "(tensor([ 3,  5,  7,  8, 11, 12, 14, 17, 19, 21, 22, 24, 25, 29, 31, 32, 34, 38,\n",
      "        39, 41, 42, 46, 47, 48, 50, 54, 56, 58, 59, 61, 62, 64, 67, 69, 70, 71,\n",
      "        72]), tensor([ 6, 10, 20, 23, 30, 37, 40, 45, 57, 60, 68]), tensor([ 4, 18, 28, 36, 44, 55, 66]), tensor([ 2, 16, 53, 65]), tensor([15, 52, 63]), tensor([13, 51]), tensor([ 9, 49]), tensor([ 1, 43]), tensor([35]), tensor([33]), tensor([27]), tensor([26]), tensor([0]))\n",
      "Traversing many trees at the same time:\n",
      "(tensor([ 3,  5,  7,  8, 11, 12, 14, 17, 19, 21, 22, 24, 25, 29, 31, 32, 34, 38,\n",
      "        39, 41, 42, 46, 47, 48, 50, 54, 56, 58, 59, 61, 62, 64, 67, 69, 70, 71,\n",
      "        72]), tensor([ 6, 10, 20, 23, 30, 37, 40, 45, 57, 60, 68]), tensor([ 4, 18, 28, 36, 44, 55, 66]), tensor([ 2, 16, 53, 65]), tensor([15, 52, 63]), tensor([13, 51]), tensor([ 9, 49]), tensor([ 1, 43]), tensor([35]), tensor([33]), tensor([27]), tensor([26]), tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Step 3: define traversal\n",
    "# ------------------------\n",
    "#\n",
    "# After defining the message passing functions, we then need to induce the\n",
    "# right order to trigger them. This is a significant departure from models\n",
    "# such as GCN, where all nodes are pulling messages from upstream ones\n",
    "# *simultaneously*.\n",
    "#\n",
    "# In the case of Tree-LSTM, messages start from leaves of the tree, and\n",
    "# propagate/processed upwards until they reach the roots. A visualization\n",
    "# is as follows:\n",
    "#\n",
    "# .. figure:: https://i.loli.net/2018/11/09/5be4b5d2df54d.gif\n",
    "#    :alt:\n",
    "#\n",
    "# DGL defines a generator to perform the topological sort, each item is a\n",
    "# tensor recording the nodes from bottom level to the roots. One can\n",
    "# appreciate the degree of parallelism by inspecting the difference of the\n",
    "# followings:\n",
    "#\n",
    "\n",
    "print('Traversing one tree:')\n",
    "print(dgl.topological_nodes_generator(a_tree))\n",
    "\n",
    "print('Traversing many trees at the same time:')\n",
    "print(dgl.topological_nodes_generator(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linbo/anaconda3/envs/nlu/lib/python3.6/site-packages/dgl/frame.py:204: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
      "  dgl_warning('Initializer is not set. Use zero initializer instead.'\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# We then call :meth:`~dgl.DGLGraph.prop_nodes` to trigger the message passing:\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "\n",
    "graph.ndata['a'] = th.ones(graph.number_of_nodes(), 1)\n",
    "graph.register_message_func(fn.copy_src('a', 'a'))\n",
    "graph.register_reduce_func(fn.sum('a', 'a'))\n",
    "\n",
    "traversal_order = dgl.topological_nodes_generator(graph)\n",
    "graph.prop_nodes(traversal_order)\n",
    "\n",
    "# the following is a syntax sugar that does the same\n",
    "# dgl.prop_nodes_topo(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# .. note::\n",
    "#\n",
    "#    Before we call :meth:`~dgl.DGLGraph.prop_nodes`, we must specify a\n",
    "#    `message_func` and `reduce_func` in advance, here we use built-in\n",
    "#    copy-from-source and sum function as our message function and reduce\n",
    "#    function for demonstration.\n",
    "#\n",
    "# Putting it together\n",
    "# -------------------\n",
    "#\n",
    "# Here is the complete code that specifies the ``Tree-LSTM`` class:\n",
    "#\n",
    "\n",
    "class TreeLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_vocabs,\n",
    "                 x_size,\n",
    "                 h_size,\n",
    "                 num_classes,\n",
    "                 dropout,\n",
    "                 pretrained_emb=None):\n",
    "        super(TreeLSTM, self).__init__()\n",
    "        self.x_size = x_size\n",
    "        self.embedding = nn.Embedding(num_vocabs, x_size)\n",
    "        if pretrained_emb is not None:\n",
    "            print('Using glove')\n",
    "            self.embedding.weight.data.copy_(pretrained_emb)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(h_size, num_classes)\n",
    "        self.cell = TreeLSTMCell(x_size, h_size)\n",
    "\n",
    "    def forward(self, batch, h, c):\n",
    "        \"\"\"Compute tree-lstm prediction given a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : dgl.data.SSTBatch\n",
    "            The data batch.\n",
    "        h : Tensor\n",
    "            Initial hidden state.\n",
    "        c : Tensor\n",
    "            Initial cell state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : Tensor\n",
    "            The prediction of each node.\n",
    "        \"\"\"\n",
    "        g = batch.graph\n",
    "        g.register_message_func(self.cell.message_func)\n",
    "        g.register_reduce_func(self.cell.reduce_func)\n",
    "        g.register_apply_node_func(self.cell.apply_node_func)\n",
    "        # feed embedding\n",
    "        embeds = self.embedding(batch.wordid * batch.mask)\n",
    "        g.ndata['iou'] = self.cell.W_iou(self.dropout(embeds)) * batch.mask.float().unsqueeze(-1)\n",
    "        g.ndata['h'] = h\n",
    "        g.ndata['c'] = c\n",
    "        # propagate\n",
    "        dgl.prop_nodes_topo(g)\n",
    "        # compute logits\n",
    "        h = self.dropout(g.ndata.pop('h'))\n",
    "        logits = self.linear(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeLSTM(\n",
      "  (embedding): Embedding(19536, 256)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (cell): TreeLSTMCell(\n",
      "    (W_iou): Linear(in_features=256, out_features=768, bias=False)\n",
      "    (U_iou): Linear(in_features=512, out_features=768, bias=False)\n",
      "    (U_f): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 00000 | Step 00000 | Loss 119.1395 | Acc 0.1233 |\n",
      "Epoch 00001 | Step 00000 | Loss 58.1637 | Acc 0.7808 |\n",
      "Epoch 00002 | Step 00000 | Loss 104.9547 | Acc 0.8356 |\n",
      "Epoch 00003 | Step 00000 | Loss 126.0810 | Acc 0.5616 |\n",
      "Epoch 00004 | Step 00000 | Loss 48.4760 | Acc 0.9041 |\n",
      "Epoch 00005 | Step 00000 | Loss 14.9707 | Acc 0.9452 |\n",
      "Epoch 00006 | Step 00000 | Loss 9.8211 | Acc 0.9726 |\n",
      "Epoch 00007 | Step 00000 | Loss 8.2823 | Acc 0.9726 |\n",
      "Epoch 00008 | Step 00000 | Loss 4.9312 | Acc 0.9863 |\n",
      "Epoch 00009 | Step 00000 | Loss 4.9256 | Acc 0.9726 |\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Main Loop\n",
    "# ---------\n",
    "#\n",
    "# Finally, we could write a training paradigm in PyTorch:\n",
    "#\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = th.device('cpu')\n",
    "# hyper parameters\n",
    "x_size = 256\n",
    "h_size = 256\n",
    "dropout = 0.5\n",
    "lr = 0.05\n",
    "weight_decay = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "# create the model\n",
    "model = TreeLSTM(trainset.num_vocabs,\n",
    "                 x_size,\n",
    "                 h_size,\n",
    "                 trainset.num_classes,\n",
    "                 dropout)\n",
    "print(model)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = th.optim.Adagrad(model.parameters(),\n",
    "                          lr=lr,\n",
    "                          weight_decay=weight_decay)\n",
    "\n",
    "def batcher(dev):\n",
    "    def batcher_dev(batch):\n",
    "        batch_trees = dgl.batch(batch)\n",
    "        return SSTBatch(graph=batch_trees,\n",
    "                        mask=batch_trees.ndata['mask'].to(device),\n",
    "                        wordid=batch_trees.ndata['x'].to(device),\n",
    "                        label=batch_trees.ndata['y'].to(device))\n",
    "    return batcher_dev\n",
    "\n",
    "train_loader = DataLoader(dataset=tiny_sst,\n",
    "                          batch_size=5,\n",
    "                          collate_fn=batcher(device),\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        g = batch.graph\n",
    "        n = g.number_of_nodes()\n",
    "        h = th.zeros((n, h_size))\n",
    "        c = th.zeros((n, h_size))\n",
    "        logits = model(batch, h, c)\n",
    "        logp = F.log_softmax(logits, 1)\n",
    "        loss = F.nll_loss(logp, batch.label, reduction='sum') \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = th.argmax(logits, 1)\n",
    "        acc = float(th.sum(th.eq(batch.label, pred))) / len(batch.label)\n",
    "        print(\"Epoch {:05d} | Step {:05d} | Loss {:.4f} | Acc {:.4f} |\".format(\n",
    "            epoch, step, loss.item(), acc))\n",
    "\n",
    "##############################################################################\n",
    "# To train the model on full dataset with different settings(CPU/GPU,\n",
    "# etc.), please refer to our repo's\n",
    "# `example <https://github.com/dmlc/dgl/tree/master/examples/pytorch/tree_lstm>`__.\n",
    "# Besides, we also provide an implementation of the Child-Sum Tree LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

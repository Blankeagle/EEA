{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server config:\n",
      "                        client\t=\t6659ea0b-c5f1-4447-9089-53bf28e041ec\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpGcaEYy/socket', 'ipc://tmpBc6wgh/socket', 'ipc://tmp4JeryZ/socket', 'ipc://tmpjYTlQH/socket', 'ipc://tmpc62g8p/socket', 'ipc://tmplCZcq8/socket', 'ipc://tmpCIt9HQ/socket', 'ipc://tmpfhu6Zy/socket']\n",
      "                worker -> sink\t=\tipc://tmpiqZ5KQ/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpPDiMGQ/socket        \n",
      "           server_current_time\t=\t2019-08-07 16:26:17.541847    \n",
      "                     statistic\t=\t{'num_data_request': 16, 'num_total_seq': 16, 'num_sys_request': 11, 'num_total_request': 27, 'num_total_client': 11, 'num_active_client': 6, 'avg_request_per_client': 2.4545454545454546, 'min_request_per_client': 1, 'max_request_per_client': 3, 'num_min_request_per_client': 2, 'num_max_request_per_client': 7, 'avg_size_per_request': 1.0, 'min_size_per_request': 1, 'max_size_per_request': 1, 'num_min_size_per_request': 1, 'num_max_size_per_request': 1, 'avg_last_two_interval': 107.24670216099912, 'min_last_two_interval': 0.012180546997115016, 'max_last_two_interval': 1541.0857134489925, 'num_min_last_two_interval': 1, 'num_max_last_two_interval': 1, 'avg_request_per_second': 25.000345254492654, 'min_request_per_second': 0.0006488931739961253, 'max_request_per_second': 82.09811925826088, 'num_min_request_per_second': 1, 'num_max_request_per_second': 1}\n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\t/tmp                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t128                           \n",
      "                     model_dir\t=\t/home/linbo/workspace/Datasets/models/BERT/chinese_L-12_H-768_A-12\n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t0                             \n",
      "                          port\t=\t8701                          \n",
      "                      port_out\t=\t8702                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tTrue                          \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '13', '1']              \n",
      "                python_version\t=\t3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.8.2                         \n",
      "                 pyzmq_version\t=\t17.1.2                        \n",
      "                   zmq_version\t=\t4.2.5                         \n",
      "             server_start_time\t=\t2019-08-07 15:57:00.004447    \n",
      "{'NULL'}\n",
      "WARNING:tensorflow:From /home/linbo/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/linbo/anaconda3/envs/nlu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 769       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,772,545\n",
      "Trainable params: 1,772,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 配置tensorflow利用显存方式\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth=True \n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_serving.client import BertClient\n",
    "from bratreader.repomodel import RepoModel\n",
    "\n",
    "\n",
    "\n",
    "def get_words(doc, bc): \n",
    "    '''get: words, embedding, spans, labels\n",
    "    '''    \n",
    "    words = []\n",
    "    wordsvec = []\n",
    "    spans = []\n",
    "    wordslabel = []\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        str_sent = sent.line\n",
    "        \n",
    "        # Embeddings of each sentence/ sequence via BERT.\n",
    "        vec = bc.encode([str_sent], show_tokens=True)\n",
    "        #print(type(vec))\n",
    "        for idx_sentence in range(len(vec[1])):\n",
    "            #print('\\n',vec[1][idx_sentence])\n",
    "            for idx_token in range(len(vec[1][idx_sentence])):\n",
    "                #print(vec[1][idx_sentence][idx_token],'\\t', vec[0][idx_sentence][idx_token][0:5])\n",
    "                \n",
    "                if( vec[1][idx_sentence][idx_token].find('[CLS]', 0, 5)==0 ):\n",
    "                    # [CLS]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('[SEP]', 0, 5)==0 ):\n",
    "                    # [SEP]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('##', 0, 2)<0 ):\n",
    "                    # Token in BERT table\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])                      \n",
    "                    start = doc.text.lower().find(words[-1], spans[-1][0])\n",
    "                    end = start + len(words[-1])\n",
    "                    spans.append([start, end])\n",
    "                    label = list(set(doc.getlabelinspan(start, end)))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel.append(label)\n",
    "                    else:\n",
    "                        wordslabel.append(['NULL'])                    \n",
    "                else:\n",
    "                    # Token started with '##' in BERT\n",
    "                    words[-1] = words[-1] + vec[1][idx_sentence][idx_token][2:]\n",
    "                    wordsvec[-1] = wordsvec[-1] + vec[0][idx_sentence][idx_token][0:]\n",
    "                    spans[-1] = ([spans[-1][0], spans[-1][0]+len(words[-1])])\n",
    "                    label = list(set(doc.getlabelinspan(spans[-1][0], spans[-1][1])))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel[-1] = label\n",
    "                    else:\n",
    "                        wordslabel[-1] = ['NULL']\n",
    "                #print(spans[-1], wordslabel[-1], words[-1], wordsvec[-1])\n",
    "    return words, wordsvec, spans, wordslabel\n",
    "\n",
    "\n",
    "def create_base_network(input_dim, nb_classes):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''    \n",
    "    sgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n",
    "    sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    N_nodes = input_dim\n",
    "    r_droupout = 0.2\n",
    "    model_base = Sequential()\n",
    "    model_base.add(Dense(N_nodes, input_shape=(input_dim,)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(nb_classes))\n",
    "    model_base.add(Activation('softmax'))    \n",
    "    model_base.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=rmsprop,\n",
    "                       metrics=['accuracy'])\n",
    "    #model_base.load_weights('model_base.h5')\n",
    "    return model_base\n",
    "\n",
    "\n",
    "def model_init(dir_data, name_file, bc):\n",
    "    '''\n",
    "    fit the model on given file with annotation \n",
    "    '''\n",
    "    corpus = RepoModel(dir_data) # load corpus\n",
    "    doc = corpus.documents[name_file] # get document with key\n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = get_words(doc, bc)\n",
    "    # wordsvec from list to array\n",
    "    wordsvec = np.asarray(wordsvec)    \n",
    "    # label encoder\n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    print(set(wordslabel))\n",
    "    \n",
    "    model = create_base_network(wordsvec[0].shape[0], len(set(wordslabel)))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_on_data(dir_data, name_file, bc):\n",
    "    '''\n",
    "    fit the model on given file with annotation \n",
    "    '''\n",
    "    corpus = RepoModel(dir_data) # load corpus\n",
    "    doc = corpus.documents[name_file] # get document with key\n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = get_words(doc, bc)\n",
    "    # wordsvec from list to array\n",
    "    wordsvec = np.asarray(wordsvec)    \n",
    "    # label encoder\n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    print('samples:', wordsvec.shape)\n",
    "    print('labels:', len(set(wordslabel)), set(wordslabel))\n",
    "    \n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(wordslabel)\n",
    "    Y_encoder = encoder.transform(wordslabel)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "    X_train, X_test, Y_train, Y_test  = wordsvec, wordsvec, Y_encoder, Y_encoder\n",
    "    \n",
    "    # model define\n",
    "    N_batch = 4\n",
    "    N_epoch = 4\n",
    "    en_verbose = 1\n",
    "    input_dim = wordsvec.shape[1]\n",
    "    N_classes = len(set(wordslabel))\n",
    "    \n",
    "    model = create_base_network(input_dim, N_classes)\n",
    "    model.summary()\n",
    "    \n",
    "    # model training\n",
    "    start   = time.time()\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=N_batch, epochs=N_epoch,\n",
    "                        verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "    end     = time.time()\n",
    "    print('time elapse training:\\t', end - start, 'sec')\n",
    "    return model\n",
    "\n",
    "\n",
    "def probs_on_data_ann(dir_data, name_file, bc, model):\n",
    "    '''\n",
    "    test the model on given file with annotation \n",
    "    '''\n",
    "    # model test\n",
    "    corpus = RepoModel(dir_data) # load corpus\n",
    "    doc = corpus.documents[name_file] # get document with key\n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = get_words(doc, bc)    \n",
    "    # wordsvec from list to array\n",
    "    wordsvec = np.asarray(wordsvec)    \n",
    "    # label encoder\n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    \n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(wordslabel)\n",
    "    Y_encoder = encoder.transform(wordslabel)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "    X_train, X_test, Y_train, Y_test  = wordsvec, wordsvec, Y_encoder, Y_encoder\n",
    "    \n",
    "    # model testing\n",
    "    start   = time.time()\n",
    "    probs = model.predict(X_test, verbose=1)\n",
    "    end     = time.time()\n",
    "    print('time elapse training:\\t', end - start, 'sec')\n",
    "    return probs\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "## setting up\n",
    "\n",
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "NAME_FILE = 'text_1'\n",
    "BERT_CLIENT = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=True)# bert model as service\n",
    "model = model_init(DIR_DATA, NAME_FILE, BERT_CLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: (381, 768)\n",
      "labels: 4 {'Company', 'QuarterlyResults', 'SalesVolume', 'NULL'}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 3076      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,774,852\n",
      "Trainable params: 1,774,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/linbo/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 381 samples, validate on 381 samples\n",
      "Epoch 1/4\n",
      "381/381 [==============================] - 1s 3ms/step - loss: 0.8593 - acc: 0.9396 - val_loss: 0.8461 - val_acc: 0.9475\n",
      "Epoch 2/4\n",
      "381/381 [==============================] - 0s 704us/step - loss: 0.8461 - acc: 0.9475 - val_loss: 0.8461 - val_acc: 0.9475\n",
      "Epoch 3/4\n",
      "381/381 [==============================] - 0s 838us/step - loss: 0.8461 - acc: 0.9475 - val_loss: 0.8461 - val_acc: 0.9475\n",
      "Epoch 4/4\n",
      "381/381 [==============================] - 0s 718us/step - loss: 0.8461 - acc: 0.9475 - val_loss: 0.8461 - val_acc: 0.9475\n",
      "time elapse training:\t 2.128357172012329 sec\n",
      "381/381 [==============================] - 0s 105us/step\n",
      "time elapse training:\t 0.04108452796936035 sec\n",
      "(381, 4)\n"
     ]
    }
   ],
   "source": [
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "MODEL_Trigger = fit_on_data(dir_data=DIR_DATA, name_file=NAME_FILE, bc=BERT_CLIENT)\n",
    "probs = probs_on_data_ann(dir_data=DIR_DATA, name_file=NAME_FILE, bc=BERT_CLIENT, model=MODEL_Trigger)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event trigger\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "\n",
    "corpus = RepoModel(DIR_DATA) # load corpus\n",
    "doc = corpus.documents[NAME_FILE] # get document with key\n",
    "bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=True) # bert model as service\n",
    "\n",
    "print(doc)\n",
    "\n",
    "words, wordsvec, spans, wordslabel = get_words_info(doc, bc)\n",
    "\n",
    "# wordsvec from list to array\n",
    "wordsvec = np.asarray(wordsvec)\n",
    "\n",
    "# label encoder\n",
    "wordslabel = [label[0] for label in wordslabel]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(wordslabel)\n",
    "Y_encoder = encoder.transform(wordslabel)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "\n",
    "# model define\n",
    "N_batch = 1\n",
    "N_epoch = 4\n",
    "en_verbose = 1\n",
    "input_dim = wordsvec.shape[1]\n",
    "N_classes = len(set(wordslabel))\n",
    "\n",
    "model = create_base_network(input_dim, N_classes)\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "print('='*65,'\\n>>training')\n",
    "start   = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=N_batch, epochs=N_epoch,\n",
    "                    verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "end     = time.time()\n",
    "print('time elapse training:\\t', end - start, 'sec') \n",
    "\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "\n",
    "# model eval\n",
    "print('='*65,'\\n>>evaluating')\n",
    "probs = model.predict(wordsvec, verbose=1)\n",
    "#Returns the loss value & metrics values for the model in test mode.\n",
    "[loss, metrics] = model.evaluate(x=wordsvec, y=Y_encoder, verbose=1)\n",
    "print('loss : ', loss)\n",
    "print(model.metrics[0], ':', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis and Retrain\n",
    "label_pred =  np.argmax(probs, axis=1)\n",
    "label_true = np.argmax(Y_encoder, axis=1)\n",
    "predict_diff = abs(label_pred - label_true)\n",
    "idx_diff = np.where(predict_diff>0)\n",
    "print(idx_diff[0])\n",
    "for idx in idx_diff[0]:\n",
    "    print(idx, '\\t', spans[idx], '\\t', wordslabel[idx], '\\t', words[idx])\n",
    "\n",
    "Times_wrong =  40\n",
    "data_wrong = np.tile(wordsvec[idx_diff[0]], (Times_wrong, 1))\n",
    "label_wrong = np.tile(Y_encoder[idx_diff[0]], (Times_wrong, 1))\n",
    "data_retrain = np.append(wordsvec, data_wrong, axis = 0)\n",
    "label_retrain = np.append(Y_encoder, label_wrong, axis =0)\n",
    "print(data_retrain.shape, label_retrain.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_retrain, label_retrain, random_state=0)\n",
    "\n",
    "# model define\n",
    "N_batch = 8\n",
    "N_epoch = 100\n",
    "en_verbose = 0\n",
    "input_dim = wordsvec.shape[1]\n",
    "N_classes = len(set(wordslabel))\n",
    "\n",
    "model = create_base_network(input_dim, N_classes)\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "print('='*65,'\\n>>training')\n",
    "start   = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=N_batch, epochs=N_epoch,\n",
    "                    verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "end     = time.time()\n",
    "print('time elapse training:\\t', end - start, 'sec') \n",
    "\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "\n",
    "# model eval\n",
    "print('='*65,'\\n>>evaluating')\n",
    "probs = model.predict(wordsvec, verbose=1)\n",
    "#Returns the loss value & metrics values for the model in test mode.\n",
    "[loss, metrics] = model.evaluate(x=wordsvec, y=Y_encoder, verbose=1)\n",
    "print('loss : ', loss)\n",
    "print(model.metrics[0], ':', metrics)\n",
    "\n",
    "\n",
    "# error analysis\n",
    "label_pred = np.argmax(probs, axis=1)\n",
    "label_true = np.argmax(Y_encoder, axis=1)\n",
    "predict_diff = abs(label_pred - label_true)\n",
    "idx_diff = np.where(predict_diff>0)\n",
    "for idx in idx_diff[0]:\n",
    "    print(idx, '\\t', spans[idx], '\\t', wordslabel[idx], '\\t', words[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 2307      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,774,083\n",
      "Trainable params: 1,774,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "================================================================= \n",
      ">>training\n",
      "WARNING:tensorflow:From /home/linbo/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 102 samples, validate on 35 samples\n",
      "Epoch 1/4\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 1.1197 - acc: 0.9314 - val_loss: 1.3816 - val_acc: 0.9143\n",
      "Epoch 2/4\n",
      "102/102 [==============================] - 0s 635us/step - loss: 1.1061 - acc: 0.9314 - val_loss: 1.3816 - val_acc: 0.9143\n",
      "Epoch 3/4\n",
      "102/102 [==============================] - 0s 597us/step - loss: 1.1061 - acc: 0.9314 - val_loss: 1.3816 - val_acc: 0.9143\n",
      "Epoch 4/4\n",
      "102/102 [==============================] - 0s 607us/step - loss: 1.1061 - acc: 0.9314 - val_loss: 1.3816 - val_acc: 0.9143\n",
      "time elapse training:\t 1.166987657546997 sec\n",
      "================================================================= \n",
      ">>testing\n",
      "35/35 [==============================] - 0s 1ms/step\n",
      "================================================================= \n",
      ">>evaluating\n",
      "35/35 [==============================] - 0s 234us/step\n",
      "loss :  1.3815510443278733\n",
      "accuracy : 0.9142857142857143\n",
      "(145, 768)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 2307      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,774,083\n",
      "Trainable params: 1,774,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "================================================================= \n",
      ">>training\n",
      "Train on 108 samples, validate on 37 samples\n",
      "Epoch 1/4\n",
      "108/108 [==============================] - 0s 3ms/step - loss: 1.0332 - acc: 0.8426 - val_loss: 0.8527 - val_acc: 0.7027\n",
      "Epoch 2/4\n",
      "108/108 [==============================] - 0s 732us/step - loss: 0.6037 - acc: 0.8241 - val_loss: 0.6071 - val_acc: 0.8919\n",
      "Epoch 3/4\n",
      "108/108 [==============================] - 0s 652us/step - loss: 0.2704 - acc: 0.9167 - val_loss: 1.0278 - val_acc: 0.8108\n",
      "Epoch 4/4\n",
      "108/108 [==============================] - 0s 589us/step - loss: 0.3260 - acc: 0.9167 - val_loss: 0.8201 - val_acc: 0.8108\n",
      "time elapse training:\t 0.7721624374389648 sec\n",
      "================================================================= \n",
      ">>testing\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "================================================================= \n",
      ">>evaluating\n",
      "37/37 [==============================] - 0s 54us/step\n",
      "loss :  0.8201429135090595\n",
      "accuracy : 0.810810811616279\n"
     ]
    }
   ],
   "source": [
    "def get_embdinspan(embds, spans, wordslabel, span):\n",
    "    for idxs in range(len(spans)):\n",
    "        s = spans[idxs]\n",
    "        if s[0]<=span[0] and span[0]<=s[1]:\n",
    "            break\n",
    "    for idxe in range(len(spans)):\n",
    "        s = spans[idxe]\n",
    "        if s[0]<=span[1] and span[1]<=s[1]:\n",
    "            break\n",
    "    idxe = idxe + 1\n",
    "    embdsin = []\n",
    "    labelsin = []\n",
    "    for idx in range(idxs,idxe):\n",
    "        embdsin.append(embds[idx])\n",
    "        labelsin.append(wordslabel[idx])\n",
    "    return embdsin, labelsin\n",
    "\n",
    "def get_embdoutspan(embds, spans, wordslabel, span):\n",
    "    for idxs in range(len(spans)):\n",
    "        s = spans[idxs]\n",
    "        if s[0]<=span[0] and span[0]<=s[1]:\n",
    "            break\n",
    "    for idxe in range(len(spans)):\n",
    "        s = spans[idxe]\n",
    "        if s[0]<=span[1] and span[1]<=s[1]:\n",
    "            break\n",
    "    embdsout = []\n",
    "    labelsout = []\n",
    "    for idx in range(0,idxs):\n",
    "        embdsout.append(embds[idx])\n",
    "        labelsout.append(wordslabel[idx])\n",
    "    for idx in range(idxe,len(wordslabel)):\n",
    "        embdsout.append(embds[idx])\n",
    "        labelsout.append(wordslabel[idx])\n",
    "    return embdsout, labelsout\n",
    "\n",
    "def get_embdintype(embds, spans, wordslabel, labelType='NULL'):\n",
    "    embdsNULL = []\n",
    "    labelsNULL = []\n",
    "    for idx in range(len(spans)):\n",
    "        if wordslabel[idx]==labelType:\n",
    "            embdsNULL.append(embds[idx])\n",
    "            labelsNULL.append(wordslabel[idx])\n",
    "    return embdsNULL, labelsNULL\n",
    "\n",
    "\n",
    "def get_events(doc, bc): \n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = get_words(doc, bc)    \n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    #print('labels:', len(set(wordslabel)), set(wordslabel))\n",
    "    \n",
    "    '''get: triggers, triggers_embedding, triggers_labels, args, args_embedding, args_labels\n",
    "    '''    \n",
    "    triggers, triggers_embedding, triggers_labels, args, args_embedding, args_labels = [], [], [], [], [], []\n",
    "    \n",
    "    for event in doc.events:\n",
    "        embdsin, labelsin = get_embdinspan(wordsvec, spans, wordslabel, event.trigger_spans)\n",
    "        for ebd in embdsin:\n",
    "            #triggers.append(event.trigger)\n",
    "            triggers_embedding.append(ebd)\n",
    "            triggers_labels.append(event.trigger_label)\n",
    "        for idx in range(len(event.args)):\n",
    "            #arg = event.args[idx]\n",
    "            span = event.args_spans[idx]\n",
    "            label = event.args_labels[idx]\n",
    "            embdsin, labelsin = get_embdinspan(wordsvec, spans, wordslabel, span)\n",
    "            for ebd in embdsin:\n",
    "                #args.append(arg)\n",
    "                args_embedding.append(ebd)\n",
    "                args_labels.append(label)\n",
    "        embdsNULL, labelsNULL = get_embdNULL(wordsvec, spans, wordslabel)\n",
    "        #print(type(triggers_embedding), type(triggers_labels), type(args_embedding), type(args_labels), type(embdsNULL), type(labelsNULL))\n",
    "        \n",
    "    return triggers, triggers_embedding, triggers_labels, args, args_embedding, args_labels, embdsNULL, labelsNULL\n",
    "\n",
    "\n",
    "# event arguments extraction\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"text_2\"\n",
    "#NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "\n",
    "corpus = RepoModel(DIR_DATA) # load corpus\n",
    "doc = corpus.documents[NAME_FILE] # get document with key\n",
    "bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=False) # bert model as service\n",
    "\n",
    "triggers, triggers_embedding, triggers_labels, args, args_embedding, args_labels, embdsNULL, labelsNULL = get_events(doc, bc)\n",
    "#print(triggers_labels, args_labels, labelsNULL)\n",
    "\n",
    "words, wordsvec, wordslabel = triggers, triggers_embedding, triggers_labels\n",
    "for idx_temp in range(len(labelsNULL)):\n",
    "    #triggers.append(event.trigger)\n",
    "    wordsvec.append(embdsNULL[idx_temp])\n",
    "    wordslabel.append(labelsNULL[idx_temp])\n",
    "\n",
    "# wordsvec from list to array\n",
    "wordsvec = np.asarray(wordsvec)\n",
    "#print(wordsvec.shape, wordslabel)\n",
    "\n",
    "# label encoder\n",
    "wordslabel = [label[0] for label in wordslabel]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(wordslabel)\n",
    "Y_encoder = encoder.transform(wordslabel)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "\n",
    "# model define\n",
    "N_batch = 4\n",
    "N_epoch = 4\n",
    "en_verbose = 1\n",
    "input_dim = wordsvec.shape[1]\n",
    "N_classes = len(set(wordslabel))\n",
    "\n",
    "model = create_base_network(input_dim, N_classes)\n",
    "model.summary()\n",
    "with open('./model.pkl', 'wb') as f:\n",
    "    pickle.dump([input_dim, N_classes], f, protocol=2)\n",
    "\n",
    "# model training\n",
    "print('='*65,'\\n>>training')\n",
    "start   = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=N_batch, epochs=N_epoch,\n",
    "                    verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "end     = time.time()\n",
    "print('time elapse training:\\t', end - start, 'sec')\n",
    "\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "\n",
    "# model eval\n",
    "print('='*65,'\\n>>evaluating')\n",
    "#Returns the loss value & metrics values for the model in test mode.\n",
    "[loss, metrics] = model.evaluate(x=X_test, y=Y_test, verbose=1)\n",
    "print('loss : ', loss)\n",
    "print(model.metrics[0], ':', metrics)\n",
    "\n",
    "model.save_weights('model_trigger.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words, wordsvec, wordslabel = args, args_embedding, args_labels\n",
    "for idx_temp in range(len(labelsNULL)):\n",
    "    #triggers.append(event.trigger)\n",
    "    wordsvec.append(embdsNULL[idx_temp])\n",
    "    wordslabel.append(labelsNULL[idx_temp])\n",
    "\n",
    "# wordsvec from list to array\n",
    "wordsvec = np.asarray(wordsvec)\n",
    "print(wordsvec.shape)\n",
    "\n",
    "# label encoder\n",
    "wordslabel = [label[0] for label in wordslabel]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(wordslabel)\n",
    "Y_encoder = encoder.transform(wordslabel)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "\n",
    "# model define\n",
    "N_batch = 4\n",
    "N_epoch = 4\n",
    "en_verbose = 1\n",
    "input_dim = wordsvec.shape[1]\n",
    "N_classes = len(set(wordslabel))\n",
    "\n",
    "model = create_base_network(input_dim, N_classes)\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "print('='*65,'\\n>>training')\n",
    "start   = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=N_batch, epochs=N_epoch,\n",
    "                    verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "end     = time.time()\n",
    "print('time elapse training:\\t', end - start, 'sec')\n",
    "\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "\n",
    "# model eval\n",
    "print('='*65,'\\n>>evaluating')\n",
    "#Returns the loss value & metrics values for the model in test mode.\n",
    "[loss, metrics] = model.evaluate(x=X_test, y=Y_test, verbose=1)\n",
    "print('loss : ', loss)\n",
    "print(model.metrics[0], ':', metrics)\n",
    "\n",
    "model.save_weights('model_arg.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 768) [['[CLS]', '中', '广', '网', '北', '京', '11', '月', '15', '日', '消', '息', '(', '记', '者', '陈', '欣', ')', '据', '中', '国', '之', '声', '《', '央', '广', '新', '闻', '》', '报', '道', '，', '经', '中', '国', '和', '巴', '基', '斯', '坦', '两', '军', '协', '商', '同', '意', '，', '[UNK]', '友', '谊', '－', '2011', '[UNK]', '中', '巴', '反', '恐', '联', '合', '训', '练', '于', '昨', '天', '(', '14', '日', ')', '起', '在', '巴', '基', '斯', '坦', '举', '行', '。', '一', '时', '间', '，', '印', '度', '媒', '体', '热', '炒', '这', '一', '话', '题', '，', '《', '印', '度', '时', '报', '》', '报', '道', '称', '，', '此', '次', '中', '巴', '军', '演', '是', '为', '了', '加', '强', '沙', '漠', '战', '能', '力', '，', '在', '边', '境', '地', '区', '向', '印', '度', '[SEP]']]\n",
      "\n",
      " ['[CLS]', '中', '广', '网', '北', '京', '11', '月', '15', '日', '消', '息', '(', '记', '者', '陈', '欣', ')', '据', '中', '国', '之', '声', '《', '央', '广', '新', '闻', '》', '报', '道', '，', '经', '中', '国', '和', '巴', '基', '斯', '坦', '两', '军', '协', '商', '同', '意', '，', '[UNK]', '友', '谊', '－', '2011', '[UNK]', '中', '巴', '反', '恐', '联', '合', '训', '练', '于', '昨', '天', '(', '14', '日', ')', '起', '在', '巴', '基', '斯', '坦', '举', '行', '。', '一', '时', '间', '，', '印', '度', '媒', '体', '热', '炒', '这', '一', '话', '题', '，', '《', '印', '度', '时', '报', '》', '报', '道', '称', '，', '此', '次', '中', '巴', '军', '演', '是', '为', '了', '加', '强', '沙', '漠', '战', '能', '力', '，', '在', '边', '境', '地', '区', '向', '印', '度', '[SEP]']\n",
      "================================================================= \n",
      ">>testing\n",
      "128/128 [==============================] - 0s 933us/step\n",
      "0\n",
      "================================================================= \n",
      ">>testing\n",
      "128/128 [==============================] - 0s 37us/step\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "# 配置tensorflow利用显存方式\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth=True \n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_serving.client import BertClient\n",
    "from bratreader.repomodel import RepoModel\n",
    "\n",
    "\n",
    "\n",
    "text = '中广网北京11月15日消息 (记者陈欣)据中国之声《央广新闻》报道，经中国和巴基斯坦两军协商同意，“友谊－2011”中巴反恐联合训练于昨天(14日)起在巴基斯坦举行。一时间，印度媒体热炒这一话题，《印度时报》报道称，此次中巴军演是为了加强沙漠战能力，在边境地区向印度施加压力。事实果真如此吗？中巴军演的目标是针对印度吗，印度媒体为何会如此解读，下面中国之声连线中央台军事记者陈欣。　　主持人：对于印度媒体热炒中巴联合军演，你有什么样的观点和评价？　　记者：首先，正像中国国防部回应所说，这次中巴联演是中巴两军的年度交流计划，并不是刻意针对某个第三国进行的。这次的代号是友谊2011，实际上此前已经有了友谊2004、友谊2006、友谊2010三次联演，分别在中巴举行，目的也很明确就是提升中巴两军的反恐作战能力。　　中国要面对东突等分裂的恐怖势力，巴基斯坦更是要应对基地和塔利班的组织。一方面，两军确实需要互相学习的，我军近年来在信息化、现代化上有很大进步可以与巴方进行交流，同时巴基斯坦参与反恐作战很多丰富的实践经验，有许多甚至是用血的代价换回来的实战经验，实战取得的经验是非常值得我军学习的。　　另一方面，反恐作战往往是依靠单独一个国家是无法完成的。很多恐怖势力的互相的联系中，中国和巴基斯坦很多恐怖势力是有彼此的联系的，需要两军联合作战。印度媒体的猜测我觉得是没有道理的，首先，从这次演习派出的人员的数量、装备包括这次科目可以看出，典型的针对防控作战需求而不是针对一个国家的。　　印度常说自己是世界大国，但如果中巴两军派出200多人，各自派出200多人的演习，印度媒体就觉得是针对它们，我觉得有点反应过渡，要么是它们信心不足，要么是它们借题发挥。印度媒体还是应该更多的尊重和互信，而不是过分的炒作对方对自己的威胁。'\n",
    "\n",
    "bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, \n",
    "                show_server_config=False)\n",
    "vec = bc.encode([text],\n",
    "                show_tokens=True)\n",
    "print(vec[0].shape, vec[1])\n",
    "for idx_sentence in range(len(vec[1])):\n",
    "    print('\\n', vec[1][idx_sentence])\n",
    "    #for idx_token in range(len(vec[1][idx_sentence])):\n",
    "        #print(vec[1][idx_sentence][idx_token],'\\t', vec[0][idx_sentence][idx_token][0:5])\n",
    "\n",
    "\n",
    "X_test = vec[0][idx_sentence]\n",
    "#print(X_test.shape, X_test)\n",
    "\n",
    "\n",
    "with open('./model.pkl', 'rb') as f:\n",
    "    structure = pickle.load(f)\n",
    "    input_dim, N_classes = structure[0], structure[1]\n",
    "    \n",
    "    \n",
    "model = create_base_network(input_dim, N_classes)\n",
    "\n",
    "model.load_weights('model_trigger.h5')\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "print(np.argmax(probs))\n",
    "\n",
    "\n",
    "\n",
    "model.load_weights('model_arg.h5')\n",
    "# model test\n",
    "print('='*65,'\\n>>testing')\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "print(np.argmax(probs))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "CLASSPATH = \"$CLASSPATH:\"\n",
    "path_standford = '/home/linbo/workspace/Datasets/Standford-coreNLP/'\n",
    "path_segmenter = path_standford + 'stanford-segmenter-2018-10-16/stanford-segmenter.jar'\n",
    "CLASSPATH = CLASSPATH + path_segmenter\n",
    "\n",
    "path_postagger = path_standford + 'stanford-postagger-full-2018-10-16/stanford-postagger.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_postagger\n",
    "\n",
    "path_ner = path_standford + 'stanford-ner-2018-10-16/stanford-ner.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_ner\n",
    "\n",
    "path_parser = path_standford + 'stanford-parser-full-2018-10-17/stanford-parser.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_parser\n",
    "\n",
    "path_parser_model = path_standford + 'stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_parser_model\n",
    "\n",
    "path_corenlp = path_standford + 'stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar:' \n",
    "CLASSPATH = CLASSPATH + ':' + path_corenlp\n",
    "\n",
    "path_model = path_standford + 'stanford-english-corenlp-2018-10-05-models.jar'\n",
    "#path_model = path_standford + 'stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_model\n",
    "\n",
    "path_api = path_standford + 'stanford-corenlp-full-2018-10-05/slf4j-api.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_api\n",
    "\n",
    "print(CLASSPATH)\n",
    "\n",
    "os.environ[\"CLASSPATH\"] = CLASSPATH\n",
    "os.environ['STANFORD_PARSER'] = path_corenlp\n",
    "os.environ['STANFORD_MODELS'] = path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Kalla, it\\'s a dog!\"\n",
    "\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "tokenizer = StanfordTokenizer()\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "class MyParser(StanfordParser):\n",
    "    def raw_parse_sents(self, sentences, verbose=False):\n",
    "        \"\"\"\n",
    "        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a\n",
    "        list of strings.\n",
    "        Each sentence will be automatically tokenized and tagged by the Stanford Parser.\n",
    "        The output format is `wordsAndTags`.\n",
    "\n",
    "        :param sentences: Input sentences to parse\n",
    "        :type sentences: list(str)\n",
    "        :rtype: iter(iter(Tree))\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            self._MAIN_CLASS,\n",
    "            '-model', self.model_path,\n",
    "            '-outputFormat', 'penn', # conll, conll2007, penn\n",
    "            '-sentences', 'newline'\n",
    "        ]\n",
    "        return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), True ))\n",
    "myparser = MyParser(model_path= path_standford + 'stanford-english-corenlp-2018-10-05-models/' \n",
    "                    + \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "en_GUI = 0\n",
    "sent = \"the quick brown fox jumps over the \\\" lazy \\\" dog .\"\n",
    "print(sent)\n",
    "res = list(myparser.raw_parse_sents([sent, sent]))\n",
    "for row in res:\n",
    "    for t in row:\n",
    "        print(type(t),'\\n',t)\n",
    "        if  en_GUI:\n",
    "            t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import BracketParseCorpusReader\n",
    "\n",
    "reader = BracketParseCorpusReader(\"./data/\", \"temp.txt\")\n",
    "for sent in reader.parsed_sents():\n",
    "    print(sent)\n",
    "t = sent\n",
    "for s in t.subtrees(lambda t: t.height() == 2): print(s)\n",
    "\n",
    "\n",
    "from nltk.tree import Tree\n",
    "\n",
    "t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
    "for s in t.subtrees(lambda t: t.height() == 2): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "class MyParser(StanfordParser):\n",
    "    def raw_parse_sents(self, sentences, verbose=False):\n",
    "        \"\"\"\n",
    "        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a\n",
    "        list of strings.\n",
    "        Each sentence will be automatically tokenized and tagged by the Stanford Parser.\n",
    "        The output format is `penn`.\n",
    "\n",
    "        :param sentences: Input sentences to parse\n",
    "        :type sentences: list(str)\n",
    "        :rtype: iter(iter(Tree))\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            self._MAIN_CLASS,\n",
    "            '-model', self.model_path,\n",
    "            '-outputFormat', 'penn', # conll, conll2007, penn\n",
    "            '-sentences', 'newline'\n",
    "        ]\n",
    "        return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), True ))\n",
    "myparser = MyParser(model_path= path_standford + 'stanford-english-corenlp-2018-10-05-models/' \n",
    "                    + \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "def load_tags(file_tags):\n",
    "    tags = OrderedDict()\n",
    "    with open(file_tags, encoding='utf-8') as ft:\n",
    "        for line in ft.readlines():\n",
    "            line = line.strip()\n",
    "            tags[line] = len(tags)\n",
    "    return tags\n",
    "\n",
    "tags = load_tags('tags.csv')\n",
    "\n",
    "sent = \"the quick brown fox jumps over the \\\" lazy \\\" dog .\"\n",
    "print(sent)\n",
    "res = list(myparser.raw_parse_sents(['1 ' + sent, '2 ' + sent]))\n",
    "for row in res:\n",
    "    for t in row: \n",
    "        x = {s[0]:tags[s.label()] for s in t.subtrees(lambda t: t.height() == 2)}\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

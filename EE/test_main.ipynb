{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_serving.client import BertClient\n",
    "from bratreader.repomodel import RepoModel\n",
    "\n",
    "# 配置tensorflow利用显存方式\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth=True \n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "def create_base_network(input_dim, nb_classes):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    N_nodes = input_dim\n",
    "    r_droupout = 0.2\n",
    "    model_base = Sequential()\n",
    "    model_base.add(Dense(N_nodes, input_shape=(input_dim,)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(nb_classes))\n",
    "    model_base.add(Activation('softmax'))\n",
    "    model_base.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=RMSprop(),\n",
    "                       metrics=['accuracy'])\n",
    "    #model_base.load_weights('model_base.h5')\n",
    "    return model_base\n",
    "\n",
    "def words_vec_label(doc, bc): \n",
    "    '''get: words, embedding, spans, labels\n",
    "    '''    \n",
    "    words = []\n",
    "    wordsvec = []\n",
    "    spans = []\n",
    "    wordslabel = []\n",
    "    \n",
    "    for str_sent in doc.text.splitlines():\n",
    "        \n",
    "        # Embeddings of each sentence/ sequence via BERT.\n",
    "        vec = bc.encode([str_sent], show_tokens=True)\n",
    "        print(type(vec))\n",
    "        for idx_sentence in range(len(vec[1])):\n",
    "            #print('\\n',vec[1][idx_sentence])\n",
    "            for idx_token in range(len(vec[1][idx_sentence])):\n",
    "                #print(vec[1][idx_sentence][idx_token],'\\t', vec[0][idx_sentence][idx_token][0:5])\n",
    "                \n",
    "                if( vec[1][idx_sentence][idx_token].find('[CLS]', 0, 5)==0 ):\n",
    "                    # [CLS]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('[SEP]', 0, 5)==0 ):\n",
    "                    # [SEP]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('##', 0, 2)<0 ):\n",
    "                    # Token in BERT table\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])                      \n",
    "                    start = doc.text.lower().find(words[-1], spans[-1][0])\n",
    "                    end = start + len(words[-1])\n",
    "                    spans.append([start, end])\n",
    "                    label = list(set(doc.getlabelinspan(start, end)))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel.append(label)\n",
    "                    else:\n",
    "                        wordslabel.append(['NULL'])                    \n",
    "                else:\n",
    "                    # Token started with '##' in BERT\n",
    "                    words[-1] = words[-1] + vec[1][idx_sentence][idx_token][2:]\n",
    "                    wordsvec[-1] = wordsvec[-1] + vec[0][idx_sentence][idx_token][0:]\n",
    "                    spans[-1] = ([spans[-1][0], spans[-1][0]+len(words[-1])])\n",
    "                    label = list(set(doc.getlabelinspan(spans[-1][0], spans[-1][1])))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel[-1] = label\n",
    "                    else:\n",
    "                        wordslabel[-1] = ['NULL']\n",
    "                #print(spans[-1], wordslabel[-1], words[-1], wordsvec[-1])\n",
    "    return words, wordsvec, spans, wordslabel\n",
    "\n",
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "def fit_on_data(dir_data=DIR_DATA, name_file=NAME_FILE):\n",
    "    '''\n",
    "    fit the model on given file with annotation \n",
    "    '''\n",
    "    corpus = RepoModel(dir_data) # load corpus\n",
    "    doc = corpus.documents[name_file] # get document with key\n",
    "    bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=True) # bert model as service\n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = words_vec_label(doc, bc)\n",
    "    \n",
    "    # wordsvec from list to array\n",
    "    wordsvec = np.asarray(wordsvec)\n",
    "    \n",
    "    # label encoder\n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(wordslabel)\n",
    "    Y_encoder = encoder.transform(wordslabel)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "    X_train, X_test, Y_train, Y_test  = wordsvec, wordsvec, Y_encoder, Y_encoder\n",
    "    \n",
    "    # model define\n",
    "    N_batch = 4\n",
    "    N_epoch = 4\n",
    "    en_verbose = 1\n",
    "    input_dim = wordsvec.shape[1]\n",
    "    N_classes = len(set(wordslabel))\n",
    "    \n",
    "    model = create_base_network(X_train[0].shape[0], len(np.unique(wordslabel)))\n",
    "    model.summary()\n",
    "    \n",
    "    # model training\n",
    "    start   = time.time()\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=N_batch, epochs=N_epoch,\n",
    "                        verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "    end     = time.time()\n",
    "    print('time elapse training:\\t', end - start, 'sec')\n",
    "    return model\n",
    "\n",
    "def probs_on_data_ann(dir_data=DIR_DATA, name_file=NAME_FILE):\n",
    "    '''\n",
    "    test the model on given file with annotation \n",
    "    '''\n",
    "    # model test\n",
    "    corpus = RepoModel(dir_data) # load corpus\n",
    "    doc = corpus.documents[name_file] # get document with key\n",
    "    bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=True) # bert model as service\n",
    "    \n",
    "    words, wordsvec, spans, wordslabel = words_vec_label(doc, bc)\n",
    "    \n",
    "    # wordsvec from list to array\n",
    "    wordsvec = np.asarray(wordsvec)\n",
    "    \n",
    "    # label encoder\n",
    "    wordslabel = [label[0] for label in wordslabel]\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(wordslabel)\n",
    "    Y_encoder = encoder.transform(wordslabel)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "    \n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "    X_train, X_test, Y_train, Y_test  = wordsvec, wordsvec, Y_encoder, Y_encoder\n",
    "    \n",
    "    # model define\n",
    "    N_batch = 4\n",
    "    N_epoch = 4\n",
    "    en_verbose = 1\n",
    "    input_dim = wordsvec.shape[1]\n",
    "    N_classes = len(set(wordslabel))\n",
    "    \n",
    "    model = create_base_network(X_train[0].shape[0], len(np.unique(wordslabel)))\n",
    "    model.summary()\n",
    "    \n",
    "    # model training\n",
    "    start   = time.time()\n",
    "    probs = model.predict(X_test, verbose=1)\n",
    "    end     = time.time()\n",
    "    print('time elapse training:\\t', end - start, 'sec')\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server config:\n",
      "                        client\t=\t94b3be19-1e3a-412c-98c4-faaba292632d\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpRqVyMN/socket', 'ipc://tmpH2y51z/socket', 'ipc://tmp97xChm/socket', 'ipc://tmpniZ9w8/socket', 'ipc://tmpRR3HMU/socket', 'ipc://tmp5jwg2G/socket', 'ipc://tmpl1GPht/socket', 'ipc://tmp3zvpxf/socket']\n",
      "                worker -> sink\t=\tipc://tmpDQBp4U/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpvpP2w1/socket        \n",
      "           server_current_time\t=\t2019-03-18 08:42:29.300647    \n",
      "                     statistic\t=\t{'num_data_request': 1, 'num_total_seq': 1, 'num_sys_request': 2, 'num_total_request': 3, 'num_total_client': 2, 'num_active_client': 1, 'avg_request_per_client': 1.5, 'min_request_per_client': 1, 'max_request_per_client': 2, 'num_min_request_per_client': 1, 'num_max_request_per_client': 1, 'avg_size_per_request': 1.0, 'min_size_per_request': 1, 'max_size_per_request': 1, 'num_min_size_per_request': 1, 'num_max_size_per_request': 1, 'avg_last_two_interval': 0.10283872694708407, 'min_last_two_interval': 0.10283872694708407, 'max_last_two_interval': 0.10283872694708407, 'num_min_last_two_interval': 1, 'num_max_last_two_interval': 1, 'avg_request_per_second': 9.723963235314578, 'min_request_per_second': 9.723963235314578, 'max_request_per_second': 9.723963235314578, 'num_min_request_per_second': 1, 'num_max_request_per_second': 1}\n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\t/tmp                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t128                           \n",
      "                     model_dir\t=\t/home/linbo/workspace/Datasets/models/BERT/uncased_L-12_H-768_A-12\n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t0                             \n",
      "                          port\t=\t8701                          \n",
      "                      port_out\t=\t8702                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tTrue                          \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '12', '0']              \n",
      "                python_version\t=\t3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.8.2                         \n",
      "                 pyzmq_version\t=\t17.1.2                        \n",
      "                   zmq_version\t=\t4.2.5                         \n",
      "             server_start_time\t=\t2019-03-18 08:41:23.170496    \n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 3076      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,774,852\n",
      "Trainable params: 1,774,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 381 samples, validate on 381 samples\n",
      "Epoch 1/4\n",
      "381/381 [==============================] - 1s 2ms/step - loss: 0.4790 - acc: 0.9265 - val_loss: 0.1460 - val_acc: 0.9764\n",
      "Epoch 2/4\n",
      "381/381 [==============================] - 0s 1ms/step - loss: 0.1813 - acc: 0.9711 - val_loss: 0.1217 - val_acc: 0.9790\n",
      "Epoch 3/4\n",
      "381/381 [==============================] - 0s 1ms/step - loss: 0.1916 - acc: 0.9816 - val_loss: 0.1504 - val_acc: 0.9869\n",
      "Epoch 4/4\n",
      "381/381 [==============================] - 0s 1ms/step - loss: 0.1313 - acc: 0.9895 - val_loss: 0.1433 - val_acc: 0.9869\n",
      "time elapse training:\t 2.2958106994628906 sec\n",
      "server config:\n",
      "                        client\t=\t074db264-cf5d-4f56-9e61-c5e3d789fa54\n",
      "                   num_process\t=\t2                             \n",
      "          ventilator -> worker\t=\t['ipc://tmpRqVyMN/socket', 'ipc://tmpH2y51z/socket', 'ipc://tmp97xChm/socket', 'ipc://tmpniZ9w8/socket', 'ipc://tmpRR3HMU/socket', 'ipc://tmp5jwg2G/socket', 'ipc://tmpl1GPht/socket', 'ipc://tmp3zvpxf/socket']\n",
      "                worker -> sink\t=\tipc://tmpDQBp4U/socket        \n",
      "           ventilator <-> sink\t=\tipc://tmpvpP2w1/socket        \n",
      "           server_current_time\t=\t2019-03-18 08:42:31.961138    \n",
      "                     statistic\t=\t{'num_data_request': 16, 'num_total_seq': 16, 'num_sys_request': 3, 'num_total_request': 19, 'num_total_client': 3, 'num_active_client': 2, 'avg_request_per_client': 6.333333333333333, 'min_request_per_client': 1, 'max_request_per_client': 16, 'num_min_request_per_client': 1, 'num_max_request_per_client': 1, 'avg_size_per_request': 1.0, 'min_size_per_request': 1, 'max_size_per_request': 1, 'num_min_size_per_request': 1, 'num_max_size_per_request': 1, 'avg_last_two_interval': 3.838545053251437, 'min_last_two_interval': 0.0093296910636127, 'max_last_two_interval': 61.14728458307218, 'num_min_last_two_interval': 1, 'num_max_last_two_interval': 1, 'avg_request_per_second': 78.18008377260963, 'min_request_per_second': 0.016353955973980188, 'max_request_per_second': 107.18468523573748, 'num_min_request_per_second': 1, 'num_max_request_per_second': 1}\n",
      "                    device_map\t=\t[]                            \n",
      "         num_concurrent_socket\t=\t8                             \n",
      "                     ckpt_name\t=\tbert_model.ckpt               \n",
      "                   config_name\t=\tbert_config.json              \n",
      "                          cors\t=\t*                             \n",
      "                           cpu\t=\tFalse                         \n",
      "            fixed_embed_length\t=\tFalse                         \n",
      "                          fp16\t=\tFalse                         \n",
      "           gpu_memory_fraction\t=\t0.5                           \n",
      "                 graph_tmp_dir\t=\t/tmp                          \n",
      "              http_max_connect\t=\t10                            \n",
      "                     http_port\t=\tNone                          \n",
      "                  mask_cls_sep\t=\tFalse                         \n",
      "                max_batch_size\t=\t256                           \n",
      "                   max_seq_len\t=\t128                           \n",
      "                     model_dir\t=\t/home/linbo/workspace/Datasets/models/BERT/uncased_L-12_H-768_A-12\n",
      "                    num_worker\t=\t1                             \n",
      "                 pooling_layer\t=\t[-2]                          \n",
      "              pooling_strategy\t=\t0                             \n",
      "                          port\t=\t8701                          \n",
      "                      port_out\t=\t8702                          \n",
      "                 prefetch_size\t=\t10                            \n",
      "           priority_batch_size\t=\t16                            \n",
      "         show_tokens_to_client\t=\tTrue                          \n",
      "               tuned_model_dir\t=\tNone                          \n",
      "                       verbose\t=\tFalse                         \n",
      "                           xla\t=\tFalse                         \n",
      "            tensorflow_version\t=\t['1', '12', '0']              \n",
      "                python_version\t=\t3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \n",
      "[GCC 7.3.0]\n",
      "                server_version\t=\t1.8.2                         \n",
      "                 pyzmq_version\t=\t17.1.2                        \n",
      "                   zmq_version\t=\t4.2.5                         \n",
      "             server_start_time\t=\t2019-03-18 08:41:23.170496    \n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 3076      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,774,852\n",
      "Trainable params: 1,774,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "381/381 [==============================] - 0s 220us/step\n",
      "time elapse training:\t 0.08519577980041504 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.24751453, 0.23398344, 0.22957015, 0.28893185],\n",
       "       [0.2055532 , 0.13576895, 0.38066113, 0.2780167 ],\n",
       "       [0.18024075, 0.11403263, 0.35785243, 0.3478742 ],\n",
       "       ...,\n",
       "       [0.20988485, 0.1656967 , 0.2577377 , 0.36668074],\n",
       "       [0.26215678, 0.06955403, 0.19814733, 0.4701419 ],\n",
       "       [0.24212164, 0.24264325, 0.24866417, 0.26657093]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "fit_on_data(dir_data=DIR_DATA, name_file=NAME_FILE)\n",
    "probs_on_data_ann(dir_data=DIR_DATA, name_file=NAME_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置tensorflow利用显存方式\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth=True \n",
    "#config.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_serving.client import BertClient\n",
    "from bratreader.repomodel import RepoModel\n",
    "\n",
    "\n",
    "def create_base_network(input_dim, nb_classes):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    N_nodes = input_dim\n",
    "    r_droupout = 0.2\n",
    "    model_base = Sequential()\n",
    "    model_base.add(Dense(N_nodes, input_shape=(input_dim,)))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(N_nodes))\n",
    "    model_base.add(Activation('relu'))\n",
    "    model_base.add(Dropout(r_droupout))\n",
    "    model_base.add(Dense(nb_classes))\n",
    "    model_base.add(Activation('softmax'))\n",
    "    model_base.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=RMSprop(),\n",
    "                       metrics=['accuracy'])\n",
    "    #model_base.load_weights('model_base.h5')    \n",
    "    return model_base\n",
    "\n",
    "def words_vec_label(doc, bc): \n",
    "    '''get: words, embedding, spans, labels\n",
    "    '''    \n",
    "    words = []\n",
    "    wordsvec = []\n",
    "    spans = []\n",
    "    wordslabel = []\n",
    "    \n",
    "    for str_sent in doc.text.splitlines():\n",
    "        \n",
    "        # Embeddings of each sentence/ sequence via BERT.\n",
    "        vec = bc.encode([str_sent], show_tokens=True)\n",
    "        for idx_sentence in range(len(vec[1])):\n",
    "            #print('\\n',vec[1][idx_sentence])\n",
    "            for idx_token in range(len(vec[1][idx_sentence])):\n",
    "                #print(vec[1][idx_sentence][idx_token],'\\t', vec[0][idx_sentence][idx_token][0:5])\n",
    "                \n",
    "                if( vec[1][idx_sentence][idx_token].find('[CLS]', 0, 5)==0 ):\n",
    "                    # [CLS]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('[SEP]', 0, 5)==0 ):\n",
    "                    # [SEP]\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])\n",
    "                    if len(spans)>0:\n",
    "                        spans.append([spans[-1][1],spans[-1][1]])\n",
    "                    else:\n",
    "                        spans.append([0,0])\n",
    "                    wordslabel.append(['NULL'])\n",
    "                elif( vec[1][idx_sentence][idx_token].find('##', 0, 2)<0 ):\n",
    "                    # Token in BERT table\n",
    "                    words.append(vec[1][idx_sentence][idx_token])\n",
    "                    wordsvec.append(vec[0][idx_sentence][idx_token][0:])                      \n",
    "                    start = doc.text.lower().find(words[-1], spans[-1][0])\n",
    "                    end = start + len(words[-1])\n",
    "                    spans.append([start, end])\n",
    "                    label = list(set(doc.getlabelinspan(start, end)))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel.append(label)\n",
    "                    else:\n",
    "                        wordslabel.append(['NULL'])                    \n",
    "                else:\n",
    "                    # Token started with '##' in BERT\n",
    "                    words[-1] = words[-1] + vec[1][idx_sentence][idx_token][2:]\n",
    "                    wordsvec[-1] = wordsvec[-1] + vec[0][idx_sentence][idx_token][0:]\n",
    "                    spans[-1] = ([spans[-1][0], spans[-1][0]+len(words[-1])])\n",
    "                    label = list(set(doc.getlabelinspan(spans[-1][0], spans[-1][1])))\n",
    "                    if len(str(label))>2:\n",
    "                        wordslabel[-1] = label\n",
    "                    else:\n",
    "                        wordslabel[-1] = ['NULL']\n",
    "                print(spans[-1], wordslabel[-1], words[-1], wordsvec[-1])\n",
    "    return words, wordsvec, spans, wordslabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generation\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DIR_DATA = \"./dataset/tmpbratfiles/\"\n",
    "NAME_FILE = \"agm_briefing_unilever_11-05-2005\"\n",
    "\n",
    "corpus = RepoModel(DIR_DATA) # load corpus\n",
    "doc = corpus.documents[NAME_FILE] # get document with key\n",
    "bc = BertClient(ip='127.0.0.1', port=8701, port_out=8702, show_server_config=True) # bert model as service\n",
    "\n",
    "words, wordsvec, spans, wordslabel = words_vec_label(doc, bc)\n",
    "\n",
    "# wordsvec from list to array\n",
    "wordsvec = np.asarray(wordsvec)\n",
    "\n",
    "# label encoder\n",
    "wordslabel = [label[0] for label in wordslabel]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(wordslabel)\n",
    "Y_encoder = encoder.transform(wordslabel)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_encoder = np_utils.to_categorical(Y_encoder)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wordsvec, Y_encoder, random_state=0)\n",
    "\n",
    "# model define\n",
    "N_batch = 4\n",
    "N_epoch = 4\n",
    "en_verbose = 1\n",
    "input_dim = wordsvec.shape[1]\n",
    "N_classes = len(set(wordslabel))\n",
    "\n",
    "model = create_base_network(X_train[0].shape[0], len(np.unique(wordslabel)))\n",
    "model.summary()\n",
    "\n",
    "# model training\n",
    "start   = time.time()\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=N_batch, epochs=N_epoch,\n",
    "                    verbose=en_verbose, validation_data=(X_test, Y_test))\n",
    "end     = time.time()\n",
    "print('time elapse training:\\t', end - start, 'sec') \n",
    "\n",
    "# model test\n",
    "probs = model.predict(X_test, verbose=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "CLASSPATH = \"$CLASSPATH:\"\n",
    "path_standford = '/home/linbo/workspace/Datasets/Standford-coreNLP/'\n",
    "path_segmenter = path_standford + 'stanford-segmenter-2018-10-16/stanford-segmenter.jar'\n",
    "CLASSPATH = CLASSPATH + path_segmenter\n",
    "\n",
    "path_postagger = path_standford + 'stanford-postagger-full-2018-10-16/stanford-postagger.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_postagger\n",
    "\n",
    "path_ner = path_standford + 'stanford-ner-2018-10-16/stanford-ner.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_ner\n",
    "\n",
    "path_parser = path_standford + 'stanford-parser-full-2018-10-17/stanford-parser.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_parser\n",
    "\n",
    "path_parser_model = path_standford + 'stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_parser_model\n",
    "\n",
    "path_corenlp = path_standford + 'stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar:' \n",
    "CLASSPATH = CLASSPATH + ':' + path_corenlp\n",
    "\n",
    "path_model = path_standford + 'stanford-english-corenlp-2018-10-05-models.jar'\n",
    "#path_model = path_standford + 'stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_model\n",
    "\n",
    "path_api = path_standford + 'stanford-corenlp-full-2018-10-05/slf4j-api.jar'\n",
    "CLASSPATH = CLASSPATH + ':' + path_api\n",
    "\n",
    "print(CLASSPATH)\n",
    "\n",
    "os.environ[\"CLASSPATH\"] = CLASSPATH\n",
    "os.environ['STANFORD_PARSER'] = path_corenlp\n",
    "os.environ['STANFORD_MODELS'] = path_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Kalla, it\\'s a dog!\"\n",
    "\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "\n",
    "tokenizer = StanfordTokenizer()\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "class MyParser(StanfordParser):\n",
    "    def raw_parse_sents(self, sentences, verbose=False):\n",
    "        \"\"\"\n",
    "        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a\n",
    "        list of strings.\n",
    "        Each sentence will be automatically tokenized and tagged by the Stanford Parser.\n",
    "        The output format is `wordsAndTags`.\n",
    "\n",
    "        :param sentences: Input sentences to parse\n",
    "        :type sentences: list(str)\n",
    "        :rtype: iter(iter(Tree))\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            self._MAIN_CLASS,\n",
    "            '-model', self.model_path,\n",
    "            '-outputFormat', 'penn', # conll, conll2007, penn\n",
    "            '-sentences', 'newline'\n",
    "        ]\n",
    "        return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), True ))\n",
    "myparser = MyParser(model_path= path_standford + 'stanford-english-corenlp-2018-10-05-models/' \n",
    "                    + \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "en_GUI = 0\n",
    "sent = \"the quick brown fox jumps over the \\\" lazy \\\" dog .\"\n",
    "print(sent)\n",
    "res = list(myparser.raw_parse_sents([sent, sent]))\n",
    "for row in res:\n",
    "    for t in row:\n",
    "        print(type(t),'\\n',t)\n",
    "        if  en_GUI:\n",
    "            t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import BracketParseCorpusReader\n",
    "\n",
    "reader = BracketParseCorpusReader(\"./data/\", \"temp.txt\")\n",
    "for sent in reader.parsed_sents():\n",
    "    print(sent)\n",
    "t = sent\n",
    "for s in t.subtrees(lambda t: t.height() == 2): print(s)\n",
    "\n",
    "\n",
    "from nltk.tree import Tree\n",
    "\n",
    "t = Tree.fromstring(\"(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))\")\n",
    "for s in t.subtrees(lambda t: t.height() == 2): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "class MyParser(StanfordParser):\n",
    "    def raw_parse_sents(self, sentences, verbose=False):\n",
    "        \"\"\"\n",
    "        Use StanfordParser to parse multiple sentences. Takes multiple sentences as a\n",
    "        list of strings.\n",
    "        Each sentence will be automatically tokenized and tagged by the Stanford Parser.\n",
    "        The output format is `penn`.\n",
    "\n",
    "        :param sentences: Input sentences to parse\n",
    "        :type sentences: list(str)\n",
    "        :rtype: iter(iter(Tree))\n",
    "        \"\"\"\n",
    "        cmd = [\n",
    "            self._MAIN_CLASS,\n",
    "            '-model', self.model_path,\n",
    "            '-outputFormat', 'penn', # conll, conll2007, penn\n",
    "            '-sentences', 'newline'\n",
    "        ]\n",
    "        return self._parse_trees_output(self._execute(cmd, '\\n'.join(sentences), True ))\n",
    "myparser = MyParser(model_path= path_standford + 'stanford-english-corenlp-2018-10-05-models/' \n",
    "                    + \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "def load_tags(file_tags):\n",
    "    tags = OrderedDict()\n",
    "    with open(file_tags, encoding='utf-8') as ft:\n",
    "        for line in ft.readlines():\n",
    "            line = line.strip()\n",
    "            tags[line] = len(tags)\n",
    "    return tags\n",
    "\n",
    "tags = load_tags('tags.csv')\n",
    "\n",
    "sent = \"the quick brown fox jumps over the \\\" lazy \\\" dog .\"\n",
    "print(sent)\n",
    "res = list(myparser.raw_parse_sents(['1 ' + sent, '2 ' + sent]))\n",
    "for row in res:\n",
    "    for t in row: \n",
    "        x = {s[0]:tags[s.label()] for s in t.subtrees(lambda t: t.height() == 2)}\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
